{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwSWq1gOETAv"
      },
      "source": [
        "# Overall Plan\n",
        "# 1. Define the Enhanced MVP Scope\n",
        "Core Features for the Netherlands-Focused MVP:\n",
        "\n",
        "## 1.1 User Input Interface\n",
        "\n",
        "Inputs:\n",
        "- Destination within the Netherlands (e.g., Amsterdam, Rotterdam)\n",
        "- Preferences (e.g., museums, outdoor activities, nightlife, culinary experiences)\n",
        "- Budget (e.g., budget-friendly, mid-range, luxury)\n",
        "- Duration (e.g., 3 days, 7 days)\n",
        "- Travel style (e.g., solo, family, romantic, adventure)\n",
        "\n",
        "## 2.1 YouTube Video Collection\n",
        "\n",
        "Fetch relevant YouTube videos specifically about traveling in the Netherlands based on user inputs.\n",
        "\n",
        "## 3.1 Video Transcription\n",
        "\n",
        "Convert video audio to text using open-source speech-to-text models.\n",
        "\n",
        "## 4.1 Content Summarization\n",
        "\n",
        "Summarize transcribed content to extract key travel tips and recommendations.\n",
        "\n",
        "## 5.1 Basic Itinerary Generation\n",
        "\n",
        "Create a day-by-day travel plan based on summarized content.\n",
        "## 6.1 Presentation of Itinerary\n",
        "\n",
        "Display the generated itinerary in a user-friendly format (e.g., web page).\n",
        "\n",
        "Out of Scope for MVP:\n",
        "- Advanced multimodal integration (video analysis, sentiment analysis)\n",
        "- Real-time interactive features (quizzes, live Q&A)\n",
        "- Detailed budget optimization with real-time pricing data\n",
        "- Visual map-based itineraries\n",
        "\n",
        "# 2. Choose the Technology Stack\n",
        "## 2.1 Backend:\n",
        "### Programming Language: Python\n",
        "### Web Framework: Streamlit (for simplicity and rapid development)\n",
        "### AI Frameworks:\n",
        "#### Hugging Face Transformers: For leveraging pre-trained language models.\n",
        "#### LangChain: For orchestrating and managing the AI workflows.\n",
        "### APIs:\n",
        "#### YouTube Data API: To fetch relevant videos.\n",
        "\n",
        "## 2.2 Frontend:\n",
        "Framework: Streamlit (handles both frontend and backend for MVP)\n",
        "\n",
        "## 2.3 Database:\n",
        "Option: SQLite or a simple JSON-based storage for MVP\n",
        "\n",
        "## 2.4 Deployment:\n",
        "Platform: Streamlit Sharing, Heroku, or a cloud platform with GPU support if needed\n",
        "\n",
        "## 2.5 Additional Tools:\n",
        "Transcription: Open-source models like Whisper available on Hugging Face or Coqui STT\n",
        "Summarization & Q&A: LLaMA 3 via Hugging Face or other open-source LMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6OoF64rEzTQ"
      },
      "source": [
        "# **Role of PyTorch in Hugging Face Transformers**\n",
        "The Hugging Face Transformers library is a popular framework for natural language processing (NLP) tasks. It supports multiple deep learning backends, with PyTorch being the primary one. Here's how PyTorch integrates with Transformers:\n",
        "\n",
        "##2.1. Model Implementation##\n",
        "Most transformer models in Hugging Face's library, including LLaMA 3, are implemented using PyTorch. When you load a model via AutoModelForCausalLM or AutoModelForSeq2SeqLM, under the hood, PyTorch is used to:\n",
        "\n",
        "Define Model Architecture: Layers, attention mechanisms, etc.\n",
        "Handle Parameters: Loading pre-trained weights, managing model parameters.\n",
        "Execute Forward Passes: Processing input data through the model to generate outputs.\n",
        "##2.2. Tokenization and Encoding##\n",
        "The tokenizer converts raw text into tokens that the model can understand. These tokens are typically represented as PyTorch tensors, which require PyTorch for further processing.\n",
        "\n",
        "##2.3. Inference and Summarization##\n",
        "When you perform tasks like summarization:\n",
        "\n",
        "Input Processing: The input text is tokenized and converted into tensors.\n",
        "Model Execution: These tensors are passed through the model to generate summaries.\n",
        "Output Decoding: The generated tokens are decoded back into human-readable text.\n",
        "All these steps are facilitated by PyTorch's tensor operations and GPU acceleration.\n",
        "\n",
        "\n",
        "\n",
        "Why is PyTorch Essential Here?\n",
        "* Model Execution: Transformers rely on PyTorch for running models efficiently, especially large ones like LLaMA 3.\n",
        "* GPU Acceleration: PyTorch enables leveraging GPUs to significantly speed up computations, making real-time or large-scale processing feasible.\n",
        "* Tensor Operations: All data manipulations (tokenization, encoding, decoding) are handled as tensor operations in PyTorch.\n",
        "* Memory Management: Efficiently manages memory, allowing you to load and run large models without exhausting system resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJjNAzt2FInm"
      },
      "source": [
        "# 1. Incorporating Google Maps for Geographical Context:\n",
        "Strategy:\n",
        "\n",
        "* Dynamic Map Embedding: For location-specific questions (like “What are the must-see attractions?” or “Where should I stay in Amsterdam?”), dynamically embed a Google Maps view with pinned locations for attractions, hotels, restaurants, etc.\n",
        "* Directions and Route Planning: Integrate the Google Maps API to show routes for getting around Amsterdam (e.g., from the user's current location to an attraction or from one attraction to another). You could provide visual maps with public transit options, biking paths, or walking directions.\n",
        "* Street View Integration: Allow users to explore attractions virtually with Google Street View to give them a more immersive experience.\n",
        "Example Use Case: When the app answers the question “What’s the best way to get around Amsterdam?”, it could display a map showing popular biking routes or public transit options with route suggestions.\n",
        "\n",
        "# 2. Using Computer Vision for Interactive Exploration:\n",
        "Strategy:\n",
        "\n",
        "* Image Classification for Amsterdam Landmarks: Integrate a computer vision model (e.g., ResNet, MobileNet) to allow users to upload or take photos, and the app identifies landmarks or attractions in Amsterdam (e.g., Rijksmuseum, Van Gogh Museum). This could be a fun feature for users to identify places they've visited or plan to visit.\n",
        "* Landmark Recommendation from Images: Let users upload a picture, and based on computer vision analysis, the app can suggest nearby attractions or provide information about the identified place.\n",
        "* Tour Planning Using Vision: Provide an option to recommend places based on images of landmarks or nature that users prefer, helping them create a personalized tour of the city.\n",
        "Example Use Case: If a user uploads a picture of an iconic canal, the app can identify it as a part of Amsterdam’s canal system and suggest nearby attractions or cafes with map integration.\n",
        "\n",
        "# 3. Integrating Text-to-Video Models like Flux:\n",
        "Strategy:\n",
        "\n",
        "* Personalized Travel Videos: For each answer (e.g., \"What are the must-see attractions in Amsterdam?\"), use Flux or another text-to-video model to generate short video clips summarizing the answer. You could feature snippets about places like the Van Gogh Museum, with animations based on the text answers.\n",
        "* Augmented Travel Guides: For each question, instead of just providing a text-based answer, generate a video guide. This video could include visuals of the locations in Amsterdam, interactive elements like maps, and recommendations that are visually appealing.\n",
        "* Visualizing Travel Routes: When answering questions about how to get from one place to another, generate videos that visually depict the route. The video could show a map, a simulated walking or biking tour, or transit routes.\n",
        "Example Use Case: For the question, “What’s the best time of year to visit Amsterdam?”, a video could be generated that shows the city in different seasons, highlighting festivals in the summer and Christmas markets in the winter.\n",
        "\n",
        "# 4. Multimodal Question Answering (QA) System:\n",
        "Strategy:\n",
        "\n",
        "* Multimodal Inputs for Travel Recommendations: Build a system where users can input not only text but also images or locations, and the app provides a combined answer. For instance, if a user provides a picture of a museum, the system can respond with historical information about the museum, the best time to visit, and even provide a Google Maps location.\n",
        "* Image-to-Text Summarization for Travel Tips: Use computer vision to extract key visual elements from an image (e.g., a picture of a busy street in Amsterdam) and combine it with text-based context to generate travel recommendations or tips.\n",
        "* Example Use Case: When users upload a photo of a place in Amsterdam, the app could suggest nearby attractions, provide historical information, and generate video content that matches the visual and textual information.\n",
        "\n",
        "# 5. Interactive Itinerary Building with Visual Elements:\n",
        "Strategy:\n",
        "\n",
        "* Interactive Maps with Itinerary Recommendations: Based on the user’s preferences and the top 10 questions, provide a drag-and-drop interactive map where they can build their itinerary. The app can suggest popular attractions, routes, and places to eat.\n",
        "* Photo-Based Itineraries: Allow users to upload photos of attractions they want to visit, and based on these, the app can generate an itinerary with accompanying visual and video content (using text-to-video models).\n",
        "* Auto-Generated Travel Videos: After users finalize their itinerary, create a personalized travel video summarizing their trip, using video content from Flux or other models, which includes key highlights and tips based on their selected locations.\n",
        "Example Use Case: After answering the question “What are the top 5 attractions in Amsterdam?”, allow users to click on each attraction, add it to their itinerary, and then generate a short personalized video that showcases the highlights of their upcoming trip.\n",
        "\n",
        "# 6. Fun and Interactive Quizzes:\n",
        "Strategy:\n",
        "\n",
        "Computer Vision-Based Quizzes: After providing the top 10 travel questions, create a fun visual quiz where users guess the name of the landmarks or attractions from images, using a computer vision model for validation.\n",
        "Interactive Map Quiz: Use Google Maps to create a quiz where users need to guess the location of famous attractions, and the app validates their choices using the Google Maps API.\n",
        "Example Use Case: After answering the question, “Where should I stay in Amsterdam?”, show a series of images of different neighborhoods and ask the user to guess the correct neighborhood.\n",
        "\n",
        "# 7. Voice Interactions and Text-to-Speech:\n",
        "Strategy:\n",
        "\n",
        "Text-to-Speech for Personalized Guides: Convert the answers to spoken text using ElevenLabs or any text-to-speech model. This adds a more interactive element, allowing users to listen to their travel guide.\n",
        "Voice-Based Travel Assistant: Instead of typing in questions, allow users to ask their questions via voice, and respond with both spoken and visual answers (e.g., maps or text-to-video content).\n",
        "Example Use Case: When a user asks, “What’s the best way to get around Amsterdam?”, the app provides a spoken response and visualizes the options on a map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgXaimRgFSLO"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8nO7ALXEFII8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#@title Install Libraries\n",
        "%pip install --upgrade --q elevenlabs==0.2.27\n",
        "%pip install --q ollama\n",
        "%pip install --q youtube-search-python youtube_transcript_api  langchain langchain_community langchain_huggingface transformers torch faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OQn8V_N9_ia"
      },
      "outputs": [],
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "!pip show ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXzgZKJx-GjU"
      },
      "outputs": [],
      "source": [
        "!ollama serve > rocama.log 2>&1 &\n",
        "!ollama pull llama3.1:8b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u4vbAPDODGJa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\bhuva\\anaconda3\\envs\\Agent\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "#@title Load Libraries\n",
        "\n",
        "# Ollama\n",
        "import ollama\n",
        "\n",
        "# DL\n",
        "import torch\n",
        "\n",
        "# Vector Databases\n",
        "import faiss\n",
        "\n",
        "# General\n",
        "import os\n",
        "import re\n",
        "import webbrowser\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "# Youtube\n",
        "from youtubesearchpython import VideosSearch\n",
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
        "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\n",
        "\n",
        "# Langchain | Elevenlabs | Langchain Agents\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import LLMChain\n",
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain_community.tools.eleven_labs.text2speech import ElevenLabsText2SpeechTool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgHhH-beHcjU"
      },
      "outputs": [],
      "source": [
        "from decimal import MIN_EMIN\n",
        "#@title Load Constants\n",
        "DESTINATION = \"Amsterdam\"\n",
        "PREFERENCES = [\"Museums\", \"Outdoor Activities\"]\n",
        "MAX_RESULTS = 20 # Number of videos to fetch\n",
        "MIN_VIEWS = 10000 # 10,000 views # Threshold for minimum views\n",
        "LLM = \"facebook/bart-large-cnn\"\n",
        "LOCAL_LLM = \"llama3\"\n",
        "MAX_TOKENS = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSiYUZmaIrRz"
      },
      "source": [
        "# Building a Pipeline Step by Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZPzUTa-fANE"
      },
      "source": [
        "##**1. Search YouTube Videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spmxIIE_ImiC"
      },
      "outputs": [],
      "source": [
        "# **Search YouTube Videos**\n",
        "\n",
        "def parse_views(views_str):\n",
        "    \"\"\"\n",
        "    Parses the views string from YouTube and converts it to an integer.\n",
        "\n",
        "    5,909 views -> 5909\n",
        "\n",
        "    \"\"\"\n",
        "    # Remove the \"views\" part and any commas\n",
        "    views_str = views_str.lower().replace('views', '').replace(',', '').strip()\n",
        "    return int(views_str)  # In case of any parsing error\n",
        "\n",
        "def fetch_youtube_videos(destination, preferences, MIN_VIEWS, MAX_RESULTS):\n",
        "  \"\"\"\n",
        "  Fetches relevant YouTube videos based on the destination and user preferences.\n",
        "\n",
        "  Parameters:\n",
        "  - destination (str): The travel destination (e.g., 'Amsterdam').\n",
        "  - preferences (list): List of user-selected preferences (e.g., ['Museums', 'Outdoor Activities']).\n",
        "  - max_results (int): Maximum number of videos to fetch.\n",
        "\n",
        "  Returns:\n",
        "  - videos (list): List of dictionaries containing video details with views parsed as integers.\n",
        "  \"\"\"\n",
        "  # Combine preferences into a search query\n",
        "  preference_query = \" \".join(preferences)\n",
        "  search_query = f\"{destination} travel guide{preference_query} Netherlands\"\n",
        "\n",
        "  # Initializing VideoSearch\n",
        "  video_search = VideosSearch(search_query, limit=MAX_RESULTS)\n",
        "\n",
        "  # Execute Search\n",
        "  search_results = video_search.result()\n",
        "\n",
        "  videos = []\n",
        "  for video in search_results['result']:\n",
        "      views_str = video['viewCount']['text']\n",
        "      views = parse_views(views_str)\n",
        "      print(f\"parseview : {views}\")\n",
        "\n",
        "      # Filter out videos with fewer than MIN_VIEWS\n",
        "      if views < MIN_VIEWS:\n",
        "          continue  # Skip this video\n",
        "\n",
        "      video_data = {\n",
        "          'Title': video['title'],\n",
        "          'Duration': video['duration'],\n",
        "          'Views': views,  # Store as integer\n",
        "          'Channel': video['channel']['name'],\n",
        "          'Link': video['link']\n",
        "        }\n",
        "      videos.append(video_data)\n",
        "\n",
        "  return videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frDvVAlLMWct"
      },
      "outputs": [],
      "source": [
        "#@title Videos List\n",
        "print(\"Fetching Relevent Youtube Videos about traveling in Amsterdam...\\n\")\n",
        "videos = fetch_youtube_videos(DESTINATION, PREFERENCES, MIN_VIEWS, MAX_RESULTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6AAH_5oM7Vc"
      },
      "outputs": [],
      "source": [
        "len(videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpiwZ0SzO52-"
      },
      "outputs": [],
      "source": [
        "videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZbjxHKGM-hh"
      },
      "outputs": [],
      "source": [
        "#@title Display Videos\n",
        "\n",
        "def display_videos(videos):\n",
        "  \"\"\"\n",
        "  Displays the list of videos in a pandas DataFrame and optionally opens them in the browser.\n",
        "\n",
        "  Parameters:\n",
        "  - videos (list): List of dictionaries containing video details.\n",
        "  \"\"\"\n",
        "  if not videos:\n",
        "    print(\"No videos found with more than 10,000 views.\")\n",
        "    return\n",
        "\n",
        "  # Create a DataFrame for better display\n",
        "  videos_df = pd.DataFrame(videos)\n",
        "  print(\"\\n Fetched YouTube Videos (Filtered by > 10,0000 views):\")\n",
        "  print(videos_df[[\"Title\", \"Duration\",\"Views\",\"Channel\"]].to_string(index = False))\n",
        "\n",
        "  # Display Clickable links\n",
        "  print(\"\\n Click the links below to watch the videos:\\n\")\n",
        "  for idx, video in enumerate(videos, 1):\n",
        "    # Display as Markdown link\n",
        "    display(HTML(f\"{idx}. <a href = '{video['Link']}' target = '_blsnk'>{video['Title']}</a>\"))\n",
        "  return videos_df\n",
        "\n",
        "videos_df = display_videos(videos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm09uI_jPBVz"
      },
      "source": [
        "We can not open Youtube Videos in Webbrowser via Google Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bcFk6b1PCh4"
      },
      "outputs": [],
      "source": [
        "# As we see We can't open Youtube videos from the colab so\n",
        "webbrowser.open('https://www.youtube.com/watch?v=zFABm07RtXk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYzV_P0IQfMn"
      },
      "outputs": [],
      "source": [
        "videos_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0eLEJRbfY2V"
      },
      "source": [
        "##**2.Transcript YouTube Videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcEWWv3cSJQY"
      },
      "outputs": [],
      "source": [
        "# **2.Transcript YouTube Videos**\n",
        "\n",
        "def extract_video_id(youtube_url):\n",
        "  \"\"\"\n",
        "  Extracts the video ID from a YouTube URL.\n",
        "\n",
        "    Parameters:\n",
        "    - youtube_url (str): The full YouTube video URL.\n",
        "\n",
        "    Returns:\n",
        "    - video_id (str): The extracted video ID.\n",
        "  \"\"\"\n",
        "\n",
        "  # Regular expression to extract video ID\n",
        "  video_id_match = re.search(r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\", youtube_url)\n",
        "  if video_id_match:\n",
        "    return video_id_match.group(1)\n",
        "  else:\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IpHIXL5S1VF"
      },
      "outputs": [],
      "source": [
        "# Example\n",
        "extract_video_id(\"https://www.youtube.com/watch?v=abcd1234EFG\")\n",
        "\n",
        "# Output: \"abcd1234EFG\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GomR6mr5TOzB"
      },
      "outputs": [],
      "source": [
        "#@title Transcription\n",
        "\n",
        "def extract_transcripts(videos_df):\n",
        "  # Initializing a new column for Transcripts\n",
        "  videos_df['Transcript'] = None\n",
        "\n",
        "  # Iterate over each video and fetch the transcript\n",
        "  for index, row in videos_df.iterrows():\n",
        "    youtube_url = row['Link']\n",
        "    video_title = row['Title']\n",
        "    video_id = extract_video_id(youtube_url)\n",
        "\n",
        "    if video_id:\n",
        "      try:\n",
        "        # Fetch the transcript using the video ID\n",
        "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages = ['en'])\n",
        "\n",
        "        # Cobine the transcript segements into a single string\n",
        "        transcript = ' '.join([segment['text'] for segment in transcript_list])\n",
        "\n",
        "        # Assign the transcript to the DataFrame\n",
        "        videos_df.at[index, \"Transcript\"] = transcript\n",
        "        print(f\"Transcript fetched for videos: '{video_title}'\")\n",
        "\n",
        "      except TranscriptsDisabled:\n",
        "        print(f\"Transcripts are disabled for this video: '{video_title}'.\")\n",
        "        videos_df.at[index, \"Transcript\"] = \"Transcripts are disabled for this video.\"\n",
        "\n",
        "      except NoTranscriptFound:\n",
        "        print(f\"No transcript found for video: '{video_title}'.\")\n",
        "        videos_df.at[index, \"Transcript\"] = \"No transcript found for this video.\"\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"An error occurred while fetching the transcript for video: '{video_title}'. Error: {e}\")\n",
        "        videos_df.at[index, \"Transcript\"] = f\"An error occurred while fetching the transcript for this video. Error: {e}\"\n",
        "    else:\n",
        "      print(f\"Could not extract video ID from URL: '{youtube_url}'.\")\n",
        "      videos_df.at[index, \"Transcript\"] = \"Invalid YouTube URL.\"\n",
        "\n",
        "  return videos_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M5maVfsVnYG"
      },
      "outputs": [],
      "source": [
        "videos_df = extract_transcripts(videos_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBIM_qEcWnzw"
      },
      "outputs": [],
      "source": [
        "videos_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4Q3U1CpfgUn"
      },
      "source": [
        "## **3.Summarization Using Facebook LLM via Hugging Face**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxTOshq1WuX0"
      },
      "outputs": [],
      "source": [
        "# **3.Summarization Using Facebook LLM via Hugging Face**\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM)\n",
        "summarizer = pipeline(\"summarization\", model = LLM)\n",
        "\n",
        "# Clean Text Function\n",
        "def clean_text(text):\n",
        "  text = re.sub(r'[^\\x00-\\x7F]+',\" \", text)\n",
        "  text = re.sub(r'\\n+',' ', text)\n",
        "  text = re.sub(r'\\s+',' ',text).strip()\n",
        "  return text\n",
        "\n",
        "def split_text_into_chunks(text, max_tokens = MAX_TOKENS):\n",
        "  tokens = tokenizer.encode(text)\n",
        "  chunks = [tokens[i:i + max_tokens]for i in range(0, len(tokens), max_tokens)]\n",
        "  return [tokenizer.decode(chunk) for chunk in chunks]\n",
        "\n",
        "# Split the Text into Chunk\n",
        "t = clean_text(videos_df['Transcript'][0])\n",
        "chunks = split_text_into_chunks(t)\n",
        "\n",
        "# Summarize each chunk\n",
        "summaries = [summarizer(chunk, max_length = 130, min_length = 30, do_sample = False)[0]['summary_text']for chunk in chunks]\n",
        "\n",
        "# Combine the summaries if need\n",
        "final_summary = \" \".join(summaries)\n",
        "final_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIjJhQPCZorQ"
      },
      "outputs": [],
      "source": [
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AdQkuUGZgcM"
      },
      "outputs": [],
      "source": [
        "len(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr1yGJOwZhw0"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.encode(t)\n",
        "print(f\"Number of Tokens: {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IVFTzUDZ5g0"
      },
      "outputs": [],
      "source": [
        "videos_df.iloc[0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHXsvqP5Z8dQ"
      },
      "outputs": [],
      "source": [
        "#@title Summarizer\n",
        "\n",
        "def split_text_into_chunsk(text, max_tokens = MAX_TOKENS):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(LLM)\n",
        "  tokens = tokenizer.encode(text)\n",
        "  chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "  return [tokenizer.decode(chunk) for chunk in chunks]\n",
        "\n",
        "def summarize_text(transcript, summarizer_pipeline):\n",
        "  \"\"\"\n",
        "  Summarizes the provided transcript using the BART model.\n",
        "\n",
        "  Parameters:\n",
        "  - transcript (str): The video transcript to summarize.\n",
        "\n",
        "  Returns:\n",
        "  - summary (str): The summarized text or an error message.\n",
        "\n",
        "  \"\"\"\n",
        "  try:\n",
        "\n",
        "    # Split the text into chunks\n",
        "    t = clean_text(transcript)\n",
        "    chunks = split_text_into_chunks(t)\n",
        "\n",
        "    # Summarizer each chunk\n",
        "    summarizer = [summarizer_pipeline(chunk, max_length = 130, min_length = 30, do_sample = False)[0]['summary_text']for chunk in chunks]\n",
        "\n",
        "    # Comibine the summaries if Needed\n",
        "    result = ' '.join(summaries)\n",
        "\n",
        "    if len(result) > 0:\n",
        "      return result\n",
        "    else:\n",
        "      return \"No summary generated.\"\n",
        "  except Exception as e:\n",
        "    # Return an error message in case of failure\n",
        "    return f\"Error summarizing transcript:  {e}\"\n",
        "\n",
        "# Initialize a new column for summaries\n",
        "videos_df['Summary'] = None\n",
        "summarizer_pipeline = pipeline(\"summarization\", model = LLM)\n",
        "\n",
        "# Iterate over each transcript and generated summaries\n",
        "for index, row in videos_df.iloc[0:1].iterrows():\n",
        "  transcript = row['Transcript']\n",
        "  video_title =row['Title']\n",
        "\n",
        "  if transcript and \"transcripts are disabled\" not in transcript.lower() and \"no transcripts found\" not in transcript.lower():\n",
        "    print(f\"Summarizing transcript for video: {video_title}\")\n",
        "    try:\n",
        "      summary = summarize_text(transcript, summarizer_pipeline)\n",
        "      videos_df.at[index, 'Summary'] = summary\n",
        "      print(f\"Summary generated for video: {video_title}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error summarizing transcript for video: {video_title}. Error: {e}\")\n",
        "      videos_df.at[index, 'Summary'] = f\"Error summarizing transcript: {e}\"\n",
        "  else:\n",
        "    print(f\"No valid transcript found for video: '{video_title}'. Skipping summarization.\\n\")\n",
        "    videos_df.at[index, 'Summary'] = \"No transcript found for this video.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egkM6NGbdaxv"
      },
      "outputs": [],
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4unHGitGdFji"
      },
      "outputs": [],
      "source": [
        "# Display the updated DataFrame with summaries\n",
        "videos_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_8LMolCeTTM"
      },
      "source": [
        "## 4.Building Dense Passage Retrieval (DPR)\n",
        "where we can efficiently retrieve the most relevant parts of the transcripts related to travel in Amsterdam. After that, we can process or summarize the retrieved passages.\n",
        "### 4.1 Set Up DPR Models (Query Encoder and Passage Encoder)\n",
        "\n",
        "Hugging Face provides DPR models with two main components:\n",
        "Query Encoder: Encodes the input query.\n",
        "Passage Encoder: Encodes the passages to be searched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKaCiJxvfqZb"
      },
      "outputs": [],
      "source": [
        "# Load DPR Question encoder and tokenizer for the query\n",
        "query_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "query_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "\n",
        "# Load DPR context encoder and tokenizer for the passages(i.e , transcript sections)\n",
        "passage_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "passage_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8RJrZrzw5dC"
      },
      "source": [
        "### 4.2 Encode Passages (Transcripts)\n",
        "We will encode the passages (i.e., parts of the transcript) and store their embeddings for efficient retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RFhJKtLw7uY"
      },
      "outputs": [],
      "source": [
        "# Encode passage (transcripts) using the DPR context encoder\n",
        "\n",
        "def encode_passage(videos_df):\n",
        "  passages = videos_df['Transcript'].tolist()\n",
        "  passage_embeddings = []\n",
        "\n",
        "  for passage in passages:\n",
        "    inputs = passage_tokenizer(passage, return_tensors = 'pt', max_length = 512, truncation = True, padding = True)\n",
        "    with torch.no_grad():\n",
        "      embedding = passage_encoder(**inputs).pooler_output\n",
        "    passage_embeddings.append(embedding.numpy())\n",
        "\n",
        "  # Convert to a Numpy Array\n",
        "  passage_embeddings = np.vstack(passage_embeddings)\n",
        "\n",
        "  return passage_embeddings\n",
        "\n",
        "# Encode all the transcripts in the DataFrame\n",
        "passage_embeddings = encode_passage(videos_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NLIOGPqyBzi"
      },
      "outputs": [],
      "source": [
        "passage_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-3Ljg0zyIRG"
      },
      "source": [
        "### 4.3 Build a FAISS Index for Fast Retrieval\n",
        "\n",
        "We'll use FAISS to index the encoded passages, making it faster to retrieve relevant passages based on the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD5MwfXDyImL"
      },
      "outputs": [],
      "source": [
        "# Initialize FAISS index for similarity search\n",
        "dimension = passage_embeddings.shape[1] # DPR embeding size is 768\n",
        "faiss_index = faiss.IndexFlatIP(dimension) # Inner Product (dot product) for similarity\n",
        "\n",
        "# Add the Passage embeddings tot he Index\n",
        "faiss_index.add(passage_embeddings)\n",
        "faiss_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oEgO4LF0MjN"
      },
      "source": [
        "### 4.4 Encode the Query and Retrieve Relevant Passages\n",
        "Next, we will encode the user's query and use it to retrieve the most relevant passages from the indexed transcripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxrRSWk80Jvs"
      },
      "outputs": [],
      "source": [
        "# Function to encode the query and search for the most relevant passages\n",
        "def search_relevant_passage(query, faiss_index, top_k =3):\n",
        "  # Encode the query using the DPR Question encoder\n",
        "  query_inputs = query_tokenizer(query, return_tensors = 'pt', max_lenght = 128, truncation = True, padding = True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    query_embedding = query_encoder(**query_inputs).pooler_output.numpy()\n",
        "  # Ensure query_embedding is 2D (i.e., shape (1, embeddingd_imension))\n",
        "  query_embedding = query_embedding.reshape(1, -1)\n",
        "  # Reshaping to (1, embedding_dimentions)\n",
        "\n",
        "  # Search for the top-k most similar packages\n",
        "  distances, indicies = faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "  print(f\"distance: {distances}\")\n",
        "  print(f\"indicies: {indicies}\")\n",
        "\n",
        "  # Retriver the corresponding transcripts and titles\n",
        "  retrived_passages = []\n",
        "  for i in range(top_k):\n",
        "    idx = indicies[0][i]\n",
        "    retrived_passages.append({\n",
        "        'Title': videos_df.iloc[idx]['Title'],# Get the title using the index\n",
        "        'Transcript': videos_df.iloc[idx]['Transcript'],# Get the transcript using the index\n",
        "        'Similarity': distances[0][i]# Get the corresponding similarity score using i, not\n",
        "    })\n",
        "\n",
        "  return retrived_passages\n",
        "\n",
        "# Example query\n",
        "query = \"What are the most best museums in Amsterdam ?\"\n",
        "\n",
        "# Searc for the most relevant passage\n",
        "retrived_passages = search_relevant_passage(query, faiss_index, top_k = 2)\n",
        "\n",
        "# Display Retrived results\n",
        "for passage in retrived_passages:\n",
        "  print(f\"Title: {passage['Title']}\")\n",
        "  print(f\"Similarity: {passage['Similarity']}\")\n",
        "  print(f\"Transcript: {passage['Transcript']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXf2B_AE8Xfj"
      },
      "outputs": [],
      "source": [
        "# Function to encode the query and search for the most relevant passages\n",
        "def search_relevant_passages(videos_df, faiss_index, query, top_k=3):\n",
        "    # Encode the query using the DPR question encoder\n",
        "    query_inputs = query_tokenizer(query, return_tensors='pt', max_length=128, truncation=True, padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = query_encoder(**query_inputs).pooler_output.numpy()\n",
        "\n",
        "    # Ensure query_embedding is 2D (i.e., shape (1, embedding_dimension))\n",
        "    query_embedding = query_embedding.reshape(1, -1)  # Reshaping to (1, embedding_dimension)\n",
        "\n",
        "    # Search for the top-k most similar passages\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "    # Filter the DataFrame based on the retrieved indices\n",
        "    top_k_indices = indices[0][:top_k]\n",
        "    top_k_videos = videos_df.iloc[top_k_indices].copy()\n",
        "    top_k_videos['Similarity Score'] = distances[0][:top_k]\n",
        "\n",
        "    # Add the query as a new column in the DataFrame\n",
        "    top_k_videos['Similarity Score'] = distances[0][:top_k]\n",
        "\n",
        "    # Add the Query as a new column in the DataFrame\n",
        "    top_k_videos['Query'] = query\n",
        "\n",
        "    return top_k_videos\n",
        "# Example query\n",
        "query = \"What are the best museums in Amsterdam ?\"\n",
        "\n",
        "# Search for the mosr relevant passage and filter videos from the videos_df\n",
        "top_k_videos = search_relevant_passages(videos_df, faiss_index, query, top_k = 3)\n",
        "\n",
        "# Display the Top_k filtered videos along with the similarity scores and query\n",
        "print(top_k_videos[['Title', 'Similarity Score', 'Query']])\n",
        "top_k_videos[['Title','Transcript','Similarity Score','Query']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fI0At26GtuK"
      },
      "outputs": [],
      "source": [
        "# Function to encode the query and search for the most relevant passages\n",
        "def search_relevant_passages(videos_df, faiss_index, query, top_k=3):\n",
        "    # Encode the query using the DPR question encoder\n",
        "    query_inputs = query_tokenizer(query, return_tensors='pt', max_length=128, truncation=True, padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        query_embedding = query_encoder(**query_inputs).pooler_output.numpy()\n",
        "\n",
        "    # Ensure query_embedding is 2D (i.e., shape (1, embedding_dimension))\n",
        "    query_embedding = query_embedding.reshape(1, -1)  # Reshaping to (1, embedding_dimension)\n",
        "\n",
        "    # Search for the top-k most similar passages\n",
        "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
        "\n",
        "    # Filter the DataFrame based on the retrieved indices\n",
        "    top_k_indicies = indices[0][:top_k]\n",
        "    top_k_videos = videos_df.iloc[top_k_indicies].copy()\n",
        "    top_k_videos['Similarity Score'] = distances[0][:top_k]\n",
        "\n",
        "    # Add the query as a new column in the DataFrame\n",
        "    top_k_videos['Similarity Score'] = distances[0][:top_k]\n",
        "\n",
        "    # Add the Query as a new column in the DataFrame\n",
        "    top_k_videos['Query'] = query\n",
        "\n",
        "    return top_k_videos\n",
        "# Example query\n",
        "query = \"What are the best museums in Amsterdam ?\"\n",
        "\n",
        "# Search for the mosr relevant passage and filter videos from the videos_df\n",
        "top_k_videos = search_relevant_passages(videos_df, faiss_index, query, top_k = 3)\n",
        "\n",
        "# Display the Top_k filtered videos along with the similarity scores and query\n",
        "print(top_k_videos[['Title', 'Similarity Score', 'Query']])\n",
        "top_k_videos[['Title','Transcript','Similarity Score','Query']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSkW14m-9zPG"
      },
      "source": [
        "## 5. Building Travel Agent to Generate Top 10 Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjV1azV49wpo"
      },
      "outputs": [],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJL6xIoL95zw"
      },
      "outputs": [],
      "source": [
        "# Function to Generate Questions using ChatLLama From Ollama\n",
        "def generate_questions(city):\n",
        "  prompt = f\"Act as an Travel Agent and Expert in {city} tour Guide. Generate a list of the top 10 questions that a first-time traveler might ask about visiting {city}.\"\n",
        "\n",
        "  # Use the Chat API to Generate response\n",
        "  response = ollama.chat(model = \"llama3.1:8b\", messages = [{'role':\"user\",\"content\":prompt}])\n",
        "  return response\n",
        "\n",
        "# Example usage\n",
        "city = 'Amsterdam'\n",
        "top_10_questions = generate_questions(city)\n",
        "print(top_10_questions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQlIwGzmH4Me"
      },
      "outputs": [],
      "source": [
        "display(Markdown(top_10_questions['message']['content']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnM7hmBVIWdk"
      },
      "outputs": [],
      "source": [
        "# Function to generate questions using LLaMA 3 via Ollama with a refined prompt\n",
        "def generate_questions(city):\n",
        "    # Refined prompt to ask specifically for only the questions, without extra text\n",
        "    prompt = f\"\"\"\n",
        "    As a travel guide expert, generate a list of the top 10 questions that a first-time traveler might ask about visiting {city}.\n",
        "    Please provide only the questions, numbered 1 to 10, without any additional descriptions.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use Ollama's Chat API to generate the questions\n",
        "    response = ollama.chat(model=\"llama3.1:8b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "\n",
        "    return response['message']['content']\n",
        "\n",
        "# Function to display the questions beautifully with Markdown\n",
        "def display_questions_with_markdown(city):\n",
        "    # Generate the top 10 questions\n",
        "    questions_text = generate_questions(city)\n",
        "\n",
        "    # Convert the text into a Markdown-friendly format\n",
        "    markdown_output = f\"### Top 10 Questions for First-Time Travelers to {city}:\\n\\n{questions_text}\"\n",
        "\n",
        "    # Display the formatted text using Markdown\n",
        "    display(Markdown(markdown_output))\n",
        "\n",
        "# Example usage\n",
        "city = \"Amsterdam\"\n",
        "display_questions_with_markdown(city)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7jnPyLPIfY_"
      },
      "source": [
        "# 5.Create Text to Speech Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYhIN08hJCa_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"ELEVEN_API_KEY\"] = userdata.get('ELEVEN_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri5d_f9HIfHP"
      },
      "outputs": [],
      "source": [
        "#@title ElevenLabs without Agents\n",
        "\n",
        "text_to_speak = '''Top 10 Questions for First-Time Travelers to Amsterdam:\n",
        "Here are the top 10 questions that a first-time traveler might ask about visiting Amsterdam:\n",
        "\n",
        "What is the best way to get around Amsterdam?\n",
        "Is Amsterdam safe for tourists?\n",
        "What are some must-see attractions in Amsterdam?\n",
        "Can I drink the tap water in Amsterdam?\n",
        "Are there any specific dress code or cultural norms I should be aware of?\n",
        "How much money do I need to budget for food and activities?\n",
        "Is Amsterdam a good place for solo travelers or couples?\n",
        "What are some popular neighborhoods or areas to stay in?\n",
        "Can I bring my own bike or rent one in Amsterdam?\n",
        "Are there any unique or quirky experiences I should have while visiting Amsterdam?'''\n",
        "\n",
        "tts = ElevenLabsText2SpeechTool()\n",
        "print(tts.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lj_cjTWIvcK"
      },
      "outputs": [],
      "source": [
        "speech_file = tts.run(text_to_speak)\n",
        "speech_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytJC2z_NKrbW"
      },
      "outputs": [],
      "source": [
        "tts.play(speech_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iVxQ0wbJiaR"
      },
      "outputs": [],
      "source": [
        "#@title ## 5.2 ElevenLAbs with Agents\n",
        "tools = load_tools([\"eleven_labs_text2speech\"])\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=LLM,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        ")\n",
        "audio_file = agent.run(text_to_speak)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Agent",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
